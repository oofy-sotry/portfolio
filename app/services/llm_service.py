"""
HuggingFace LLM ì„œë¹„ìŠ¤ (KoGPT2, KoBART)
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
import os
import re

class LLMService:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” - ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self.embedding_model = None
        self.generation_model = None
        self.summarization_model = None
        self.tokenizer = None
        self.summarization_tokenizer = None
        
        # ëª¨ë¸ ë¡œë”©
        self._load_models()
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
        self._check_models_loaded()
    
    def _load_models(self):
        """ë¡œì»¬ ê²½ëŸ‰í™” ëª¨ë¸ ë¡œë”©"""
        try:
            models_dir = "/app/models"
            
            # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
            print("ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            embedding_path = os.path.join(models_dir, "embedding_model")
            if os.path.exists(embedding_path):
                try:
                self.embedding_model = SentenceTransformer(embedding_path)
                print("âœ… ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ ê°€ëŠ¥): {e}")
                    print("   HuggingFaceì—ì„œ ìµœì‹  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
                    # ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
                    import shutil
                    try:
                        shutil.rmtree(embedding_path)
                        print(f"   ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì‚­ì œ: {embedding_path}")
                    except:
                        pass
                    self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            else:
                print("âš ï¸ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©...")
                self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            
            # 2. ìƒì„± ëª¨ë¸ ë¡œë”©
            print("ğŸ”„ ë¡œì»¬ ìƒì„± ëª¨ë¸ ë¡œë”© ì¤‘...")
            generation_path = os.path.join(models_dir, "generation_model")
            if os.path.exists(generation_path):
                self.tokenizer = AutoTokenizer.from_pretrained(generation_path)
                self.generation_model = AutoModelForCausalLM.from_pretrained(generation_path)
                self.generation_model.to(self.device)
                # pad_token ì„¤ì • (distilgpt2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                print("âœ… ë¡œì»¬ ìƒì„± ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                print("âš ï¸ ë¡œì»¬ ìƒì„± ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©...")
                self.tokenizer = AutoTokenizer.from_pretrained('distilgpt2')
                self.generation_model = AutoModelForCausalLM.from_pretrained('distilgpt2')
                self.generation_model.to(self.device)
                # pad_token ì„¤ì • (distilgpt2ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                print("âœ… ê¸°ë³¸ ìƒì„± ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            # 3. ìš”ì•½ ëª¨ë¸ ë¡œë”©
            print("ğŸ”„ ë¡œì»¬ ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘...")
            summarization_path = os.path.join(models_dir, "summarization_model")
            if os.path.exists(summarization_path):
                self.summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_path)
                self.summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_path)
                self.summarization_model.to(self.device)
                print("âœ… ë¡œì»¬ ìš”ì•½ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                print("âš ï¸ ë¡œì»¬ ìš”ì•½ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©...")
                self.summarization_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')
                self.summarization_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')
                self.summarization_model.to(self.device)
            
            print("ğŸ‰ ëª¨ë“  ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ”„ ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œë¡œ ì „í™˜...")
            # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œë¡œ ì „í™˜
            self.embedding_model = None
            self.generation_model = None
            self.summarization_model = None
    
    def _check_models_loaded(self):
        """ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸ ë° ë¡œê·¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸:")
        print("="*60)
        print(f"  ì„ë² ë”© ëª¨ë¸: {'âœ… ë¡œë“œë¨' if self.embedding_model is not None else 'âŒ ë¡œë“œ ì‹¤íŒ¨'}")
        print(f"  ìƒì„± ëª¨ë¸: {'âœ… ë¡œë“œë¨' if self.generation_model is not None else 'âŒ ë¡œë“œ ì‹¤íŒ¨'}")
        print(f"  ìƒì„± í† í¬ë‚˜ì´ì €: {'âœ… ë¡œë“œë¨' if self.tokenizer is not None else 'âŒ ë¡œë“œ ì‹¤íŒ¨'}")
        print(f"  ìš”ì•½ ëª¨ë¸: {'âœ… ë¡œë“œë¨' if self.summarization_model is not None else 'âŒ ë¡œë“œ ì‹¤íŒ¨'}")
        print("="*60)
        
        if self.generation_model is None or self.tokenizer is None:
            print("âš ï¸ ê²½ê³ : ìƒì„± ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   LLM ì‘ë‹µ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•˜ë©°, ê¸°ë³¸ ì‘ë‹µë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
            print("   ì›ì¸ í™•ì¸:")
            print("   1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ í™•ì¸")
            print("   2. ë©”ëª¨ë¦¬ ë¶€ì¡± í™•ì¸")
            print("   3. Docker ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸: docker compose logs web")
        print()
    
    def get_embeddings(self, texts):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if self.embedding_model is None:
            return None
        
        try:
            embeddings = self.embedding_model.encode(texts)
            return embeddings
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def summarize_text(self, text, max_length=100):
        """í…ìŠ¤íŠ¸ ìš”ì•½"""
        if self.summarization_model is None or self.summarization_tokenizer is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ìš”ì•½
            return text[:max_length] + "..." if len(text) > max_length else text
        
        try:
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            inputs = self.summarization_tokenizer(
                text,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors="pt"
            ).to(self.device)
            
            # ìš”ì•½ ìƒì„±
            with torch.no_grad():
                summary_ids = self.summarization_model.generate(
                    inputs.input_ids,
                    max_length=max_length,
                    min_length=30,
                    length_penalty=2.0,
                    num_beams=4,
                    early_stopping=True
                )
            
            summary = self.summarization_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            return summary
            
        except Exception as e:
            print(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:max_length] + "..." if len(text) > max_length else text
    
    def generate_response(self, prompt, max_length=150, mode="concise"):
        """ì‘ë‹µ ìƒì„±"""
        if self.generation_model is None or self.tokenizer is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ
            print(f"âš ï¸ LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            print(f"   generation_model: {self.generation_model is not None}")
            print(f"   tokenizer: {self.tokenizer is not None}")
            return self._get_fallback_response(prompt)
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬
            if mode == "concise":
                formatted_prompt = f"ì§ˆë¬¸: {prompt}\në‹µë³€:"
            else:
                formatted_prompt = f"ì§ˆë¬¸: {prompt}\nìƒì„¸í•œ ë‹µë³€:"
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256
            ).to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    inputs.input_ids,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if "ë‹µë³€:" in response:
                response = response.split("ë‹µë³€:")[-1].strip()
            if "ìƒì„¸í•œ ë‹µë³€:" in response:
                response = response.split("ìƒì„¸í•œ ë‹µë³€:")[-1].strip()
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not response or len(response.strip()) == 0:
                return self._get_fallback_response(prompt)
            
            return response
            
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return self._get_fallback_response(prompt)
    
    def _get_fallback_response(self, prompt):
        """ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        fallback_responses = {
            "ìê¸°ì†Œê°œ": "ì•ˆë…•í•˜ì„¸ìš”! í’€ìŠ¤íƒ ê°œë°œìì…ë‹ˆë‹¤. Python, Flask, JavaScript ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•©ë‹ˆë‹¤.",
            "ê¸°ìˆ ìŠ¤íƒ": "ì£¼ìš” ê¸°ìˆ ìŠ¤íƒ: Python, Flask, JavaScript, HTML/CSS, MySQL, Docker, Git",
            "í”„ë¡œì íŠ¸": "ì´ í¬íŠ¸í´ë¦¬ì˜¤ ì‚¬ì´íŠ¸ëŠ” Flaskë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë°œí•œ í’€ìŠ¤íƒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.",
            "ì—°ë½ì²˜": "ì´ë©”ì¼ì´ë‚˜ ì—°ë½ì²˜ ì •ë³´ëŠ” ì—°ë½ì²˜ í˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ê²½ë ¥": "ì›¹ ê°œë°œ ê²½í—˜ê³¼ ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹¤ë¬´ ì—­ëŸ‰ì„ ìŒ“ì•„ì™”ìŠµë‹ˆë‹¤."
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword, response in fallback_responses.items():
            if keyword in prompt:
                return response
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ ì—°ë½ì²˜ í˜ì´ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    def get_similarity_score(self, text1, text2):
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if self.embedding_model is None:
            return 0.0
        
        try:
            embeddings = self.embedding_model.encode([text1, text2])
            similarity = torch.cosine_similarity(
                torch.tensor(embeddings[0]).unsqueeze(0),
                torch.tensor(embeddings[1]).unsqueeze(0)
            )
            return float(similarity[0])
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
